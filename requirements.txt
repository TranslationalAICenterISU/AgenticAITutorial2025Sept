# Core dependencies
openai>=1.0.0
anthropic>=0.8.0
python-dotenv>=1.0.0

# LangChain ecosystem
langchain>=0.1.0
langchain-openai>=0.0.5
langchain-anthropic>=0.1.0
langchain-community>=0.0.20
langchain-experimental>=0.0.50
langgraph>=0.0.40

# CrewAI
crewai>=0.22.0
crewai-tools>=0.1.0

# Google AI
google-generativeai>=0.3.0

# SmolAgents (HuggingFace official framework)
smolagents[toolkit]>=0.1.0

# Core ML dependencies (used by multiple frameworks)
transformers>=4.35.0
torch>=2.0.0

# DSPy
dspy-ai>=2.4.0

# Additional utilities
requests>=2.31.0
pydantic>=2.5.0
tiktoken>=0.5.0
numpy>=1.24.0
pandas>=2.0.0

# Development tools
jupyter>=1.0.0
ipython>=8.0.0
pytest>=7.4.0
black>=23.0.0
flake8>=6.0.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0

# API and web tools
fastapi>=0.104.0
uvicorn>=0.24.0
httpx>=0.25.0

# Memory and vector stores
chromadb>=0.4.0
faiss-cpu>=1.7.4

# Monitoring and logging
wandb>=0.16.0

# Local Models and Small LMs
ollama>=0.1.0
huggingface-hub>=0.20.0
transformers>=4.35.0
accelerate>=0.24.0
bitsandbytes>=0.41.0  # For quantization
torch>=2.0.0
torchvision>=0.15.0
sentencepiece>=0.1.99
tokenizers>=0.15.0

# Local inference optimization
optimum>=1.14.0
auto-gptq>=0.4.0  # GPU quantization
ctransformers>=0.2.0  # CPU inference
llama-cpp-python>=0.2.0  # CPU inference for Llama models

# Optional GPU acceleration
# torch-audio>=2.0.0  # Uncomment if needed
# xformers>=0.0.22  # Uncomment for memory efficiency

# Local model management
litellm>=1.0.0  # Unified API for local models
instructor>=0.4.0  # Structured outputs

# Model Context Protocol (MCP)
# Note: MCP is evolving - check https://github.com/modelcontextprotocol for latest packages
mcp>=0.1.0  # MCP SDK (when available)
asyncio-mqtt>=0.11.0  # For MCP transport layers
websockets>=11.0  # WebSocket transport
aiohttp>=3.8.0  # HTTP transport
psutil>=5.9.0  # System information tools

# Additional protocol support
json-rpc>=1.15.0  # JSON-RPC implementation
pydantic>=2.5.0  # Data validation for MCP messages